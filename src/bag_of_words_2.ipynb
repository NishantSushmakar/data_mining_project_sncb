{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0cd7646-7a70-44de-99f1-c46f7e0ea651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "import config\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9eb6486a-5ee1-4bfa-a52e-c808bcf72f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(row):\n",
    "    data = dict()\n",
    "    for seconds, vehicle_id, event_id, ac_state, dc_state, train_speed in zip(\n",
    "        row[\"seconds_to_incident_sequence\"],\n",
    "        row[\"vehicles_sequence\"],\n",
    "        row[\"events_sequence\"],\n",
    "        row[\"dj_ac_state_sequence\"],\n",
    "        row[\"dj_dc_state_sequence\"],\n",
    "        row[\"train_kph_sequence\"],\n",
    "    ):\n",
    "        if seconds not in data.keys():\n",
    "            data[seconds] = {\n",
    "                vehicle_id: {\n",
    "                    event_id: {\n",
    "                        \"train_speed\": train_speed,\n",
    "                        \"ac_state\": ac_state,\n",
    "                        \"dc_state\": dc_state,\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        elif vehicle_id not in data[seconds].keys():\n",
    "            data[seconds] = {\n",
    "                vehicle_id: {\n",
    "                    event_id: {\n",
    "                        \"train_speed\": train_speed,\n",
    "                        \"ac_state\": ac_state,\n",
    "                        \"dc_state\": dc_state,\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        elif event_id not in data[seconds][vehicle_id].keys():\n",
    "            data[seconds][vehicle_id] = {\n",
    "                event_id: {\n",
    "                    \"train_speed\": train_speed,\n",
    "                    \"ac_state\": ac_state,\n",
    "                    \"dc_state\": dc_state,\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            print(\"problem\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "class ConvertSequencesToLists(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, sequence_columns):\n",
    "        self.sequence_columns = sequence_columns\n",
    "        self.output_column = \"sequence_dict\"\n",
    "\n",
    "    def process_data(self, X):\n",
    "        for col in self.sequence_columns:\n",
    "            X[col] = X[col].apply(ast.literal_eval)\n",
    "        ## Creating dictionary with the column    \n",
    "        X[self.output_column] = X.apply(lambda row: make_dict(row), axis=1)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self,X,y=None):\n",
    "        \n",
    "        return self.process_data(X)\n",
    "\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        return self.process_data(X)\n",
    "    \n",
    "\n",
    "class RemoveNoise(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.frequency_dict = {}\n",
    "        self.classes_frequency_dict = {}\n",
    "        self.r_event_dict = {}\n",
    "        \n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def get_frequency_dict(self,X):\n",
    "        \n",
    "        lst_event_sequence = X['events_sequence'].tolist()\n",
    "\n",
    "        frequency_dict = {}\n",
    "\n",
    "        for lst in lst_event_sequence:\n",
    "\n",
    "            for event_id in lst: \n",
    "                if event_id not in frequency_dict.keys():\n",
    "                    frequency_dict[event_id] = 1\n",
    "                else:\n",
    "                    frequency_dict[event_id] += 1\n",
    "\n",
    "\n",
    "        return frequency_dict\n",
    "    \n",
    "\n",
    "    def get_classes_frequency_dict(self,X):\n",
    "\n",
    "        classes_frequency_dict = {}\n",
    "\n",
    "        for incident_type in list(X['incident_type'].value_counts().keys()): \n",
    "\n",
    "            sub_lst_event_sequence = X[X['incident_type']==incident_type]['events_sequence'].tolist()   \n",
    "\n",
    "            if incident_type not in classes_frequency_dict.keys():\n",
    "                classes_frequency_dict[incident_type] = {}\n",
    "            \n",
    "            for lst in sub_lst_event_sequence:\n",
    "                for event_id in lst: \n",
    "                    if event_id not in classes_frequency_dict[incident_type].keys():\n",
    "                        classes_frequency_dict[incident_type][event_id] = 1\n",
    "                    else:\n",
    "                        classes_frequency_dict[incident_type][event_id] += 1\n",
    "\n",
    "\n",
    "        return classes_frequency_dict\n",
    "    \n",
    "\n",
    "    def get_relevance(self):\n",
    "\n",
    "        r_event_dict = {}\n",
    "\n",
    "        for key, nested_dict in self.classes_frequency_dict.items():\n",
    "\n",
    "            if key not in r_event_dict.keys():\n",
    "                r_event_dict[key] = {}\n",
    "\n",
    "            for event_id, value in nested_dict.items():\n",
    "                r_event_dict[key][event_id] = value/self.frequency_dict[event_id]\n",
    "\n",
    "        return r_event_dict\n",
    "    \n",
    "\n",
    "    def get_index_to_drop(self,x,incident_type,threshold=0.15):\n",
    "        index_lst = []\n",
    "\n",
    "        for i,value in enumerate(x):\n",
    "            \n",
    "            if value in self.r_event_dict[incident_type].keys():\n",
    "                if self.r_event_dict[incident_type][value] > threshold:\n",
    "                    index_lst.append(i)\n",
    "\n",
    "\n",
    "        return index_lst\n",
    "    \n",
    "    def get_selected_data(seld,selected_index_lst,col_lst):\n",
    "\n",
    "        if len(selected_index_lst)==0:\n",
    "            return np.nan\n",
    "        \n",
    "        new_lst = [col_lst[i] for i in selected_index_lst]\n",
    "\n",
    "        return new_lst\n",
    "    \n",
    "\n",
    "    \n",
    "    def fit_transform(self,X,y=None):\n",
    "\n",
    "        self.frequency_dict = self.get_frequency_dict(X)\n",
    "        self.classes_frequency_dict = self.get_classes_frequency_dict(X)\n",
    "        self.r_event_dict = self.get_relevance()\n",
    "\n",
    "        joblib.dump(self.r_event_dict,config.R_EVENT_DICT_PATH)\n",
    "\n",
    "        X['index_to_select'] = X[['incident_type','events_sequence']].apply(lambda row:self.get_index_to_drop(row['events_sequence'],row['incident_type']),axis=1)\n",
    "        for col in config.SEQUENCE_COLUMNS:\n",
    "            X[f'{col}'] = X[[col,'index_to_select']].apply(lambda row:self.get_selected_data(row['index_to_select'],row[col]),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        X = X.dropna().reset_index(drop=True)\n",
    "        X = X.drop(columns =['index_to_select'])\n",
    "\n",
    "        return X \n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        self.r_event_dict = joblib.load(config.R_EVENT_DICT_PATH)\n",
    "\n",
    "        X['index_to_select'] = X[['incident_type','events_sequence']].apply(lambda row:self.get_index_to_drop(row['events_sequence'],row['incident_type']),axis=1)\n",
    "        for col in config.SEQUENCE_COLUMNS:\n",
    "            X[f'{col}'] = X[[col,'index_to_select']].apply(lambda row:self.get_selected_data(row['index_to_select'],row[col]),axis=1)\n",
    "    \n",
    "\n",
    "        X = X.dropna().reset_index(drop=True)\n",
    "        X = X.drop(columns =['index_to_select'])\n",
    "\n",
    "        return X \n",
    "\n",
    "class OutlierImputer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, lat_extreme, lon_extreme, n_neighbors=2, model_file=os.path.join(config.ENCODER_PATH,\"knn_imputer.pkl\")):\n",
    "        self.lat_extreme = lat_extreme\n",
    "        self.lon_extreme = lon_extreme\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.model_file = model_file\n",
    "        self.knn_imputer = None\n",
    "\n",
    "    def mark_outliers(self, X):\n",
    "        #outliers := NaN\n",
    "        X.loc[(X[\"approx_lat\"] < self.lat_extreme[0]) | (X[\"approx_lat\"] > self.lat_extreme[1]), \"approx_lat\"] = None\n",
    "        X.loc[(X[\"approx_lon\"] < self.lon_extreme[0]) | (X[\"approx_lon\"] > self.lon_extreme[1]), \"approx_lon\"] = None\n",
    "        return X\n",
    "\n",
    "    def apply_imputer(self, group):\n",
    "        features = group[[\"approx_lat\", \"approx_lon\"]]\n",
    "        imputed = self.knn_imputer.transform(features)\n",
    "        group[[\"approx_lat\", \"approx_lon\"]] = imputed\n",
    "        return group\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self,X,y=None):\n",
    "        X = self.mark_outliers(X)\n",
    "        features = X[[\"approx_lat\", \"approx_lon\"]]\n",
    "        self.knn_imputer = KNNImputer(n_neighbors=self.n_neighbors)\n",
    "        self.knn_imputer.fit(features)\n",
    "\n",
    "        joblib.dump(self.knn_imputer, self.model_file)\n",
    "        #we still need to fill in the outliers' fields even for train data\n",
    "        X = X.groupby(\"incident_type\").apply(self.apply_imputer)\n",
    "        X = X.reset_index(drop=True)\n",
    "        X = X.sort_values(by=X.columns[0]).reset_index(drop=True)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        #outliers := NaN\n",
    "        X = self.mark_outliers(X)\n",
    "        if self.knn_imputer is None:\n",
    "            self.knn_imputer = joblib.load(self.model_file)\n",
    "\n",
    "        X = X.groupby(\"incident_type\").apply(self.apply_imputer)\n",
    "        X = X.reset_index(drop=True)\n",
    "        X = X.sort_values(by=X.columns[0]).reset_index(drop=True)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "class AddIndexSequence(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_idx(self, ls):\n",
    "\n",
    "        return_idx = len(ls)-1\n",
    "           \n",
    "        for idx in range(0, len(ls) - 1):\n",
    "            if (int(ls[idx + 1]) > 0):\n",
    "                return idx\n",
    "            \n",
    "        return return_idx\n",
    "\n",
    "    def process_data(self, X):\n",
    "        X[\"index_sequence\"] = X[\"seconds_to_incident_sequence\"].apply(self.get_idx)\n",
    "        return X\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.process_data(X)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.process_data(X)\n",
    "\n",
    "class AddWindowIndicesModified(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, window_start=-3600, window_end=0):\n",
    "        self.window_start = window_start\n",
    "        self.window_end = window_end\n",
    "\n",
    "    def find_range_indexes(self,numbers, start_integer, end_integer):\n",
    "        \n",
    "        # Find the first valid index\n",
    "        start_index = None\n",
    "        for i in range(len(numbers)):\n",
    "            if start_integer <= numbers[i] <= end_integer:\n",
    "                start_index = i\n",
    "                break\n",
    "        \n",
    "        # If no start index found, return None\n",
    "        if start_index is None:\n",
    "            return np.nan,np.nan\n",
    "        \n",
    "        # Find the last valid index\n",
    "        end_index = start_index\n",
    "        for j in range(start_index, len(numbers)):\n",
    "            if start_integer <= numbers[j] <= end_integer:\n",
    "                end_index = j\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return int(start_index), int(end_index)\n",
    "\n",
    "    def process_data(self, X):\n",
    "        \n",
    "        X[[\"window_min_idx\", \"window_max_idx\"]] = X[\"seconds_to_incident_sequence\"].apply(\n",
    "            lambda x: self.find_range_indexes(x,self.window_start,self.window_end)\n",
    "        ).apply(pd.Series)\n",
    "\n",
    "\n",
    "        X = X.dropna(subset=[\"window_min_idx\"]).reset_index(drop=True)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.process_data(X)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.process_data(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AddWindowIndices(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, window_start=3600, window_end=600):\n",
    "        self.window_start = window_start\n",
    "        self.window_end = window_end\n",
    "\n",
    "    def get_window(self, lst, idx):\n",
    "        # Pre-incident window\n",
    "        pre_incident_idx = idx\n",
    "        while abs(int(lst[pre_incident_idx])) <= self.window_start and pre_incident_idx >= 0:\n",
    "            pre_incident_idx -= 1\n",
    "        pre_incident_idx += 1\n",
    "\n",
    "        # Post-incident window\n",
    "        post_incident_idx = idx\n",
    "        while post_incident_idx < len(lst) and int(lst[post_incident_idx]) <= self.window_end:\n",
    "            post_incident_idx += 1\n",
    "        post_incident_idx -= 1\n",
    "\n",
    "        return pre_incident_idx, post_incident_idx\n",
    "\n",
    "    def process_data(self, X):\n",
    "        X[[\"window_min_idx\", \"window_max_idx\"]] = X[\n",
    "            [\"seconds_to_incident_sequence\", \"index_sequence\"]\n",
    "        ].apply(\n",
    "            lambda row: self.get_window(row[\"seconds_to_incident_sequence\"], row[\"index_sequence\"]),\n",
    "            axis=1,\n",
    "        ).apply(pd.Series)\n",
    "        return X\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.process_data(X)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.process_data(X)\n",
    "\n",
    "class AddWindowedSequences(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def get_data_sequence_within_windows(self, row, column):\n",
    "        \n",
    "        min_idx = int(row[\"window_min_idx\"])\n",
    "        max_idx = int(row[\"window_max_idx\"])\n",
    "        return row[column][min_idx : max_idx + 1]\n",
    "\n",
    "    def process_data(self, X):\n",
    "        for col in self.columns:\n",
    "            X[f\"window_{col}\"] = X.apply(\n",
    "                lambda row: self.get_data_sequence_within_windows(row, col), axis=1\n",
    "            )\n",
    "            \n",
    "        return X\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \n",
    "        return self.process_data(X)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.process_data(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee0711bc-80ad-46ac-a833-5bfba1750ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramCounter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_n=5,vectoriser_path=\"bow_vectoriser.pkl\"):\n",
    "        self.max_n = max_n\n",
    "        self.model_file = os.path.join(config.ENCODER_PATH,vectoriser_path)  \n",
    "        \n",
    "    def count_ngrams(self, event_sequences):\n",
    "        # Convert the event sequences to space-separated strings\n",
    "        sequences = [' '.join(map(str, seq)) for seq in event_sequences if seq]  # Filter out empty sequences\n",
    "        \n",
    "        if not sequences:\n",
    "            return defaultdict(lambda: defaultdict(int))  # Return an empty ngram count if no sequences are left\n",
    "        \n",
    "        ngram_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "        # Loop through the range of n-grams (1-gram to max_n-gram)\n",
    "        for n in range(1, self.max_n + 1):\n",
    "            vectorizer = CountVectorizer(ngram_range=(n, n))\n",
    "            ngram_matrix = vectorizer.fit_transform(sequences)\n",
    "            ngram_tokens = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            for ngram, count in zip(ngram_tokens, ngram_matrix.sum(axis=0).A1):\n",
    "                ngram_counts[n][ngram] += count\n",
    "    \n",
    "        return ngram_counts\n",
    "\n",
    "    def get_max_ngrams(self, ngram_counts):\n",
    "        # Extract n-grams with maximum counts for each n\n",
    "        max_ngrams = {}\n",
    "        for n, ngrams in ngram_counts.items():\n",
    "            if ngrams:  # Ensure there are n-grams for this n\n",
    "                max_ngram = max(ngrams.items(), key=lambda x: x[1])\n",
    "                max_ngrams[n] = {max_ngram[0]: max_ngram[1]}\n",
    "        return max_ngrams\n",
    "\n",
    "    def process_data(self, X):\n",
    "        # Apply n-gram counting and extract max n-grams for each incident type\n",
    "        # X = X[X[\"incident_type\"].isin([16, 6, 3, 7])]\n",
    "\n",
    "        max_ngrams_per_type = {}\n",
    "        \n",
    "        for incident_type in X['incident_type'].unique():\n",
    "            event_sequences = X[X['incident_type'] == incident_type]['window_events_sequence']\n",
    "            ngram_counts = self.count_ngrams(event_sequences)\n",
    "            max_ngrams_per_type[incident_type] = self.get_max_ngrams(ngram_counts)\n",
    "        # for incident_type in X['incident_type'].unique():\n",
    "#             event_sequences = X[X['incident_type'] == incident_type]['events_sequence']\n",
    "#             ngram_counts_per_type[incident_type] = self.count_ngrams(event_sequences)\n",
    "\n",
    "#         # Add the n-gram counts as a new column in the DataFrame\n",
    "        X['ngram_counts'] = X['incident_type'].map(max_ngrams_per_type)\n",
    "        joblib.dump(max_ngrams_per_type,self.model_file)\n",
    "        \n",
    "        return X\n",
    "\n",
    "        # return max_ngrams_per_type\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.process_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ef7605c-7e73-4d42-acd7-f27f7de2a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveEvent2956(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column='events_sequence', event_to_remove=2956):\n",
    "        self.column = column\n",
    "        self.event_to_remove = event_to_remove\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Ensure the column contains lists and remove the specified event\n",
    "        X[self.column] = X[self.column].apply(\n",
    "            lambda seq: [event for event in seq if event != self.event_to_remove]\n",
    "            if isinstance(seq, list) else seq\n",
    "        )\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08d830f5-ca6c-408d-88c9-5d833bca6d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/hadiqa/data_mining_project_sncb/data/sncb_data_challenge.csv\", delimiter=\";\")\n",
    "df = df[df[\"incident_type\"].isin([16, 6, 3, 7])]\n",
    "# # Remove 2956 from event_sequence\n",
    "# df[\"events_sequence\"] = df[\"events_sequence\"].apply(\n",
    "#     lambda x: [event for event in eval(x) if event != 2956]\n",
    "# )\n",
    "\n",
    "# df.to_csv('aaaaa.csv', index=False)\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for incident_type, group in df.groupby('incident_type'):\n",
    "    if len(group) > 0:\n",
    "        # Randomly select one row for the test dataset\n",
    "        test_row = group.sample(n=1, random_state=42)\n",
    "        test_data.append(test_row)\n",
    "        \n",
    "        # The rest go to the train dataset\n",
    "        train_rows = group.drop(test_row.index)\n",
    "        train_data.append(train_rows)\n",
    "\n",
    "# Concatenate train and test data\n",
    "train_data = pd.concat(train_data, ignore_index=True)\n",
    "test_data = pd.concat(test_data, ignore_index=True)\n",
    "\n",
    "data_cleaning_pipeline = Pipeline(steps=[\n",
    "    (\"convert_sequences\", ConvertSequencesToLists(sequence_columns=config.SEQUENCE_COLUMNS)),\n",
    "    (\"remove_noise\", RemoveNoise()),\n",
    "    (\"impute_outlier\", OutlierImputer(lat_extreme=[49.5072, 51.4978], lon_extreme=[2.5833, 6.3667], n_neighbors=2)),\n",
    "    (\"find_index\", AddIndexSequence()),\n",
    "    (\"find_window_index\", AddWindowIndices(window_start=9600, window_end=600)),\n",
    "    (\"remove_event_2956\", RemoveEvent2956(column=\"events_sequence\", event_to_remove=2956)),  # New step\n",
    "    (\"windowed_sequences_creation\", AddWindowedSequences(columns=config.SEQUENCE_COLUMNS)),\n",
    "    (\"ngram_counts\", NgramCounter(max_n=5)),  # Added n-gram counting step\n",
    "])\n",
    "\n",
    "df_train = data_cleaning_pipeline.fit_transform(train_data)\n",
    "df_test=data_cleaning_pipeline.transform(test_data)\n",
    "output_file_train = \"sncb_data_transformed_features_train.csv\"\n",
    "output_file_test = \"sncb_data_transformed_features_test.csv\"\n",
    "df_train.to_csv(output_file_train, index=False, sep=\";\")\n",
    "df_test.to_csv(output_file_test, index=False, sep=\";\")\n",
    "\n",
    "# print(f\"Transformed data saved to {output_file}\")\n",
    "# selected_columns = ['incident_type', 'ngram_counts']\n",
    "\n",
    "# # Filter the DataFrame to only show those columns\n",
    "# filtered_df = df_train[selected_columns]\n",
    "\n",
    "# print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff956b-a99b-46dd-892f-aaaea9cf8749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026bffba-a6e0-4681-9daa-ad8048a150e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc72a61-d1cb-45d9-876d-4950ca03c484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
