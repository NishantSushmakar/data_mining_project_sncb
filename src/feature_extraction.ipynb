{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "625e5e83-4d01-4b52-8211-d033778f524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffee8e73-fea5-4ccf-b128-c2828a627ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data saved to sncb_data_transformed_features.csv\n"
     ]
    }
   ],
   "source": [
    "def make_dict(row):\n",
    "    data = dict()\n",
    "    for seconds, vehicle_id, event_id, ac_state, dc_state, train_speed in zip(\n",
    "        row[\"window_seconds_to_incident_sequence\"],\n",
    "        row[\"window_vehicles_sequence\"],\n",
    "        row[\"window_events_sequence\"],\n",
    "        row[\"window_dj_ac_state_sequence\"],\n",
    "        row[\"window_dj_dc_state_sequence\"],\n",
    "        row[\"window_train_kph_sequence\"],\n",
    "    ):\n",
    "        if seconds not in data.keys():\n",
    "            data[seconds] = {\n",
    "                vehicle_id: {\n",
    "                    event_id: {\n",
    "                        \"train_speed\": train_speed,\n",
    "                        \"ac_state\": ac_state,\n",
    "                        \"dc_state\": dc_state,\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        elif vehicle_id not in data[seconds].keys():\n",
    "            data[seconds] = {\n",
    "                vehicle_id: {\n",
    "                    event_id: {\n",
    "                        \"train_speed\": train_speed,\n",
    "                        \"ac_state\": ac_state,\n",
    "                        \"dc_state\": dc_state,\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        elif event_id not in data[seconds][vehicle_id].keys():\n",
    "            data[seconds][vehicle_id] = {\n",
    "                event_id: {\n",
    "                    \"train_speed\": train_speed,\n",
    "                    \"ac_state\": ac_state,\n",
    "                    \"dc_state\": dc_state,\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            print(\"problem\")\n",
    "    return data\n",
    "\n",
    "class ConvertSequencesToLists(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, sequence_columns):\n",
    "        self.sequence_columns = sequence_columns\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for col in self.sequence_columns:\n",
    "            X[col] = X[col].apply(ast.literal_eval)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "class OutlierImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lat_extreme, lon_extreme, n_neighbors=2):\n",
    "        self.lat_extreme = lat_extreme\n",
    "        self.lon_extreme = lon_extreme\n",
    "        self.n_neighbors = n_neighbors\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X.loc[\n",
    "            (X[\"approx_lat\"] < self.lat_extreme[0]) | (X[\"approx_lat\"] > self.lat_extreme[1]),\n",
    "            \"approx_lat\",\n",
    "        ] = None\n",
    "        X.loc[\n",
    "            (X[\"approx_lon\"] < self.lon_extreme[0]) | (X[\"approx_lon\"] > self.lon_extreme[1]),\n",
    "            \"approx_lon\",\n",
    "        ] = None\n",
    "\n",
    "        def impute_group(group):\n",
    "            features = group[[\"approx_lat\", \"approx_lon\"]]\n",
    "            imputer = KNNImputer(n_neighbors=self.n_neighbors)\n",
    "            imputed = imputer.fit_transform(features)\n",
    "            group[[\"approx_lat\", \"approx_lon\"]] = imputed\n",
    "            return group\n",
    "\n",
    "        X = X.groupby(\"incident_type\").apply(impute_group)\n",
    "        X = X.reset_index(drop=True)\n",
    "        X = X.sort_values(by=X.columns[0]).reset_index(drop=True)\n",
    "\n",
    "        return X\n",
    "\n",
    "class AddIndexSequence(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_idx(self, ls):\n",
    "        for idx in range(0, len(ls) - 1):\n",
    "            if int(ls[idx + 1]) > 0:\n",
    "                return idx\n",
    "        return idx + 1\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X[\"index_sequence\"] = X[\"seconds_to_incident_sequence\"].apply(self.get_idx)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "class AddWindowIndices(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, window_start=3600, window_end=600):\n",
    "        self.window_start = window_start\n",
    "        self.window_end = window_end\n",
    "\n",
    "    def get_window(self, lst, idx):\n",
    "        pre_incident_idx = idx\n",
    "        while abs(int(lst[pre_incident_idx])) <= self.window_start and pre_incident_idx >= 0:\n",
    "            pre_incident_idx -= 1\n",
    "        pre_incident_idx += 1\n",
    "\n",
    "        post_incident_idx = idx\n",
    "        while post_incident_idx < len(lst) and int(lst[post_incident_idx]) <= self.window_end:\n",
    "            post_incident_idx += 1\n",
    "        post_incident_idx -= 1\n",
    "\n",
    "        return pre_incident_idx, post_incident_idx\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X[[\"window_min_idx\", \"window_max_idx\"]] = X[\n",
    "            [\"seconds_to_incident_sequence\", \"index_sequence\"]\n",
    "        ].apply(\n",
    "            lambda row: self.get_window(row[\"seconds_to_incident_sequence\"], row[\"index_sequence\"]),\n",
    "            axis=1,\n",
    "        ).apply(pd.Series)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "class AddWindowedSequences(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def get_data_sequence_within_windows(self, row, column):\n",
    "        min_idx = row[\"window_min_idx\"]\n",
    "        max_idx = row[\"window_max_idx\"]\n",
    "        return row[column][min_idx : max_idx + 1]\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for col in self.columns:\n",
    "            X[f\"window_{col}\"] = X.apply(\n",
    "                lambda row: self.get_data_sequence_within_windows(row, col), axis=1\n",
    "            )\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "class AddDictionaryColumn(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, sequence_columns, output_column=\"dictionary_column\"):\n",
    "        self.sequence_columns = sequence_columns\n",
    "        self.output_column = output_column\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X[self.output_column] = X.apply(lambda row: make_dict(row), axis=1)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "class CalculateMeanMedianSpeed(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, dict_column=\"dictionary_column\"):\n",
    "        self.dict_column = dict_column\n",
    "\n",
    "    def compute_mean_and_median_speed(self, data_dict):\n",
    "        speeds = []\n",
    "        for seconds_data in data_dict.values():\n",
    "            for vehicle_data in seconds_data.values():\n",
    "                for event_data in vehicle_data.values():\n",
    "                    speeds.append(event_data[\"train_speed\"])\n",
    "        if speeds:\n",
    "            return np.mean(speeds), np.median(speeds)\n",
    "        else:\n",
    "            return np.nan, np.nan\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X[[\"mean_train_speed\", \"median_train_speed\"]] = X[self.dict_column].apply(\n",
    "            self.compute_mean_and_median_speed\n",
    "        ).apply(pd.Series)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "class EventSequenceNGrams(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, sequence_column='window_events_sequence', max_ngram=3):\n",
    "        self.sequence_column = sequence_column\n",
    "        self.max_ngram = max_ngram\n",
    "        self.vectorizers = {}\n",
    "        self.top_ngrams = {}\n",
    "    \n",
    "    def prepare_sequences(self, sequences):\n",
    "        return [' '.join(map(str, seq)) for seq in sequences]\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        sequences = self.prepare_sequences(X[self.sequence_column])\n",
    "        \n",
    "        for n in range(1, self.max_ngram + 1):\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                ngram_range=(n, n),\n",
    "                lowercase=False, \n",
    "                token_pattern=r'(?u)\\b\\w+\\b'  \n",
    "            )\n",
    "            self.vectorizers[n] = vectorizer\n",
    "            self.vectorizers[n].fit(sequences)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def get_top_ngrams(self, sequence, n):\n",
    "        sequence_str = [' '.join(map(str, sequence))]        \n",
    "        tfidf_matrix = self.vectorizers[n].transform(sequence_str)\n",
    "        feature_names = self.vectorizers[n].get_feature_names_out()\n",
    "        nonzero = tfidf_matrix.nonzero()\n",
    "        scores = zip(nonzero[1], tfidf_matrix.data)\n",
    "        sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        if sorted_scores:\n",
    "            top_idx = sorted_scores[0][0]\n",
    "            return feature_names[top_idx]\n",
    "        return None\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()        \n",
    "        for n in range(1, self.max_ngram + 1):\n",
    "            X[f'top_{n}_gram'] = X[self.sequence_column].apply(\n",
    "                lambda seq: self.get_top_ngrams(seq, n)\n",
    "            )\n",
    "        \n",
    "        return X\n",
    "\n",
    "class StatesCombinationCounter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ac_column='window_dj_ac_state_sequence', dc_column='window_dj_dc_state_sequence'):\n",
    "        self.ac_column = ac_column\n",
    "        self.dc_column = dc_column\n",
    "    \n",
    "    def count_state_combinations(self, row):\n",
    "        ac_states = row[self.ac_column]\n",
    "        dc_states = row[self.dc_column]        \n",
    "        counts = {\n",
    "            'true_true_count': 0,\n",
    "            'true_false_count': 0,\n",
    "            'false_true_count': 0,\n",
    "            'false_false_count': 0\n",
    "        }\n",
    "        for ac, dc in zip(ac_states, dc_states):\n",
    "            if ac and dc:\n",
    "                counts['true_true_count'] += 1\n",
    "            elif ac and not dc:\n",
    "                counts['true_false_count'] += 1\n",
    "            elif not ac and dc:\n",
    "                counts['false_true_count'] += 1\n",
    "            else:\n",
    "                counts['false_false_count'] += 1\n",
    "        \n",
    "        return pd.Series(counts)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        combination_counts = X.apply(self.count_state_combinations, axis=1)\n",
    "        \n",
    "        for column in ['true_true_count', 'true_false_count', 'false_true_count', 'false_false_count']:\n",
    "            X[f'ac_dc_{column}'] = combination_counts[column]\n",
    "        \n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "class MostFrequentIncidentLocationWithClustering(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lat_column='approx_lat', lon_column='approx_lon', incident_type_column='incident_type', eps=0.01, min_samples=5):\n",
    "        self.lat_column = lat_column\n",
    "        self.lon_column = lon_column\n",
    "        self.incident_type_column = incident_type_column\n",
    "        self.eps = eps  \n",
    "        self.min_samples = min_samples \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        lat_lon = X_copy[[self.lat_column, self.lon_column]].values\n",
    "\n",
    "        db = DBSCAN(eps=self.eps, min_samples=self.min_samples, metric='haversine')\n",
    "        X_copy['cluster'] = db.fit_predict(np.radians(lat_lon))  # DBSCAN expects radians for geographical coordinates\n",
    "\n",
    "        def get_most_frequent_cluster(group):\n",
    "            most_frequent_cluster = group['cluster'].value_counts().idxmax()\n",
    "            cluster_points = group[group['cluster'] == most_frequent_cluster]\n",
    "            centroid_lat = np.mean(cluster_points[self.lat_column])\n",
    "            centroid_lon = np.mean(cluster_points[self.lon_column])\n",
    "            return centroid_lat, centroid_lon\n",
    "\n",
    "        most_frequent_locations = X_copy.groupby(self.incident_type_column).apply(get_most_frequent_cluster)\n",
    "\n",
    "        X_copy[['most_frequent_lat', 'most_frequent_lon']] = X_copy[self.incident_type_column].map(most_frequent_locations).apply(pd.Series)\n",
    "\n",
    "        return X_copy\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(\"data/sncb_data_challenge.csv\", delimiter=\";\")\n",
    "\n",
    "    sequence_columns = [\n",
    "        \"vehicles_sequence\",\n",
    "        \"events_sequence\",\n",
    "        \"seconds_to_incident_sequence\",\n",
    "        \"train_kph_sequence\",\n",
    "        \"dj_ac_state_sequence\",\n",
    "        \"dj_dc_state_sequence\",\n",
    "    ]\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"convert_sequences\", ConvertSequencesToLists(sequence_columns=sequence_columns)),\n",
    "            (\"outlier_imputer\", OutlierImputer(lat_extreme=[49.5072, 51.4978], lon_extreme=[2.5833, 6.3667], n_neighbors=2)),\n",
    "            (\"add_index_sequence\", AddIndexSequence()),\n",
    "            (\"add_window_indices\", AddWindowIndices(window_start=3600, window_end=600)),\n",
    "            (\"add_windowed_sequences\", AddWindowedSequences(columns=sequence_columns)),\n",
    "            (\"add_dictionary_and_mean_median_speed\", Pipeline([\n",
    "                (\"add_dictionary\", AddDictionaryColumn(sequence_columns=sequence_columns)),\n",
    "                (\"compute_mean_and_median_speed\", CalculateMeanMedianSpeed(dict_column=\"dictionary_column\")),\n",
    "            ])),\n",
    "            (\"event_sequence_ngrams\", EventSequenceNGrams(sequence_column=\"window_events_sequence\", max_ngram=3)),\n",
    "            (\"state_combinations\", StatesCombinationCounter()),\n",
    "            (\"most_frequent_event_location\", MostFrequentIncidentLocationWithClustering()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df_transformed = pipeline.fit_transform(df)\n",
    "    output_file = \"sncb_data_transformed_features.csv\"\n",
    "    df_transformed.to_csv(output_file, index=False, sep=\";\")\n",
    "    print(f\"Transformed data saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c49b9-e686-4748-9591-e731432ea9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4866d944-7185-472b-88e8-9cad29073069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
